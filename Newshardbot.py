import os
import re
import time
import sqlite3
import json
from datetime import datetime, timezone, timedelta
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup

try:
    from zoneinfo import ZoneInfo
except Exception:
    ZoneInfo = None


BOT_TOKEN = os.getenv("TG_BOT_TOKEN")
CHANNEL = os.getenv("TG_CHANNEL")

# –ü–∞—Ä—Å–∏–º –í–°–ï –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –æ–±—â–µ–π –ª–µ–Ω—Ç—ã
NEWS_INDEX_URL = "https://worldoftanks.eu/ru/news/"

DB_PATH = "posted.sqlite3"
UA = "Mozilla/5.0 (WoTEUNewsBot/2.0)"
HTTP_TIMEOUT = 30

# /ru/news/<category>/<slug>/
ARTICLE_PATH_RE = re.compile(r"^/ru/news/([^/]+)/([^/]+)/?$")

# ===== –ù–∞—Å—Ç—Ä–æ–π–∫–∏ =====
MAX_POSTS_PER_RUN = 2            # –º–∞–∫—Å–∏–º—É–º –ø–æ—Å—Ç–æ–≤ –∑–∞ –∑–∞–ø—É—Å–∫
WINDOW_HOURS = 48                # –ø—É–±–ª–∏–∫—É–µ–º –¢–û–õ–¨–ö–û –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 48 —á–∞—Å–æ–≤
PAGES_TO_SCAN = 6                # —Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü –ª–µ–Ω—Ç—ã –º–∞–∫—Å–∏–º—É–º —Å–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –∑–∞ –∑–∞–ø—É—Å–∫
ARTICLES_PER_PAGE_HINT = 48      # –ø–æ–¥—Å–∫–∞–∑–∫–∞/–ª–∏–º–∏—Ç –Ω–∞ —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫ (–∑–∞—â–∏—Ç–∞)
SLEEP_BETWEEN_POSTS_SEC = 2
# =====================


# ---------- DB ----------
def init_db():
    con = sqlite3.connect(DB_PATH)
    cur = con.cursor()
    cur.execute(
        """
        CREATE TABLE IF NOT EXISTS posted(
            url TEXT PRIMARY KEY,
            title TEXT,
            tag TEXT,
            posted_at TEXT
        )
        """
    )
    con.commit()
    con.close()


def already_posted(url: str) -> bool:
    con = sqlite3.connect(DB_PATH)
    cur = con.cursor()
    cur.execute("SELECT 1 FROM posted WHERE url=?", (url,))
    row = cur.fetchone()
    con.close()
    return row is not None


def mark_posted(url: str, title: str, tag: str):
    con = sqlite3.connect(DB_PATH)
    cur = con.cursor()
    cur.execute(
        """
        INSERT OR IGNORE INTO posted(url, title, tag, posted_at)
        VALUES(?, ?, ?, ?)
        """,
        (url, title, tag, datetime.now(timezone.utc).isoformat()),
    )
    con.commit()
    con.close()


# ---------- Helpers ----------
def html_escape(s: str) -> str:
    return (
        s.replace("&", "&amp;")
        .replace("<", "&lt;")
        .replace(">", "&gt;")
        .replace('"', "&quot;")
    )


def tg_api_post(method: str, data: dict):
    api = f"https://api.telegram.org/bot{BOT_TOKEN}/{method}"
    r = requests.post(api, data=data, timeout=HTTP_TIMEOUT)
    if not r.ok:
        try:
            payload = r.json()
        except Exception:
            payload = {"raw": r.text}
        raise SystemExit(
            "Telegram API error\n"
            f"HTTP: {r.status_code}\n"
            f"Method: {method}\n"
            f"Response: {payload}"
        )
    return r


def make_button(url: str):
    return json.dumps(
        {"inline_keyboard": [[{"text": "üîó –ß–∏—Ç–∞—Ç—å –Ω–æ–≤–æ—Å—Ç—å", "url": url}]]},
        ensure_ascii=False,
    )


def tg_send_photo(photo_url: str, caption_html: str, button_url: str):
    tg_api_post(
        "sendPhoto",
        {
            "chat_id": CHANNEL,
            "photo": photo_url,
            "caption": caption_html,
            "parse_mode": "HTML",
            "disable_notification": "false",
            "reply_markup": make_button(button_url),
        },
    )


def tg_send_message(text_html: str, button_url: str):
    # –ü—Ä–µ–≤—å—é –æ—Å—Ç–∞–≤–ª—è–µ–º –≤–∫–ª—é—á—ë–Ω–Ω—ã–º ‚Äî Telegram –ø–æ–¥—Ç—è–Ω–µ—Ç –∫–∞—Ä—Ç–∏–Ω–∫—É –ø–æ —Å—Å—ã–ª–∫–µ, –µ—Å–ª–∏ —Å–º–æ–∂–µ—Ç
    tg_api_post(
        "sendMessage",
        {
            "chat_id": CHANNEL,
            "text": text_html,
            "parse_mode": "HTML",
            "disable_web_page_preview": "false",
            "reply_markup": make_button(button_url),
        },
    )


def parse_iso_datetime(s: str):
    if not s:
        return None
    s = s.strip()
    if s.endswith("Z"):
        s = s[:-1] + "+00:00"
    try:
        dt = datetime.fromisoformat(s)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except Exception:
        return None


def extract_from_jsonld(soup: BeautifulSoup):
    for script in soup.find_all("script", attrs={"type": "application/ld+json"}):
        raw = script.get_text(strip=True)
        if not raw:
            continue
        try:
            data = json.loads(raw)
        except Exception:
            continue

        objs = data if isinstance(data, list) else [data]
        for obj in objs:
            if not isinstance(obj, dict):
                continue
            dp = obj.get("datePublished") or obj.get("dateCreated")
            if isinstance(dp, str):
                dt = parse_iso_datetime(dp)
                if dt:
                    return dt
    return None


def fetch_article_meta(article_url: str):
    """
    –û–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –∫ —Å—Ç–∞—Ç—å–µ: –¥–æ—Å—Ç–∞—ë–º
    - published_dt (UTC)
    - image_url (og/twitter/itemprop)
    - title (–µ—Å–ª–∏ –Ω–∞–¥–æ)
    """
    r = requests.get(article_url, headers={"User-Agent": UA}, timeout=HTTP_TIMEOUT)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "html.parser")

    # ---- date ----
    published_dt = None

    meta = soup.find("meta", attrs={"property": "article:published_time"})
    if meta and meta.get("content"):
        published_dt = parse_iso_datetime(meta["content"])

    if not published_dt:
        t = soup.find("time")
        if t and t.get("datetime"):
            published_dt = parse_iso_datetime(t["datetime"])

    if not published_dt:
        meta2 = soup.find("meta", attrs={"itemprop": "datePublished"})
        if meta2 and meta2.get("content"):
            published_dt = parse_iso_datetime(meta2["content"])

    if not published_dt:
        published_dt = extract_from_jsonld(soup)

    # ---- image ----
    image_url = None

    og = soup.find("meta", attrs={"property": "og:image"})
    if og and og.get("content"):
        image_url = og["content"].strip()

    if not image_url:
        tw = soup.find("meta", attrs={"name": "twitter:image"})
        if tw and tw.get("content"):
            image_url = tw["content"].strip()

    if not image_url:
        ip = soup.find("meta", attrs={"itemprop": "image"})
        if ip and ip.get("content"):
            image_url = ip["content"].strip()

    # ---- title fallback ----
    title = None
    ot = soup.find("meta", attrs={"property": "og:title"})
    if ot and ot.get("content"):
        title = ot["content"].strip()

    if not title:
        if soup.title and soup.title.get_text(strip=True):
            title = soup.title.get_text(strip=True)

    return published_dt, image_url, title


def normalize_url(u: str) -> str:
    # –£–±–∏—Ä–∞–µ–º —è–∫–æ—Ä—è/–ø–∞—Ä–∞–º–µ—Ç—Ä—ã, —á—Ç–æ–±—ã URL –≤ –±–∞–∑–µ –±—ã–ª —Å—Ç–∞–±–∏–ª—å–Ω—ã–π
    p = urlparse(u)
    clean = p._replace(query="", fragment="")
    return clean.geturl()


def tag_from_article_url(article_url: str) -> str:
    p = urlparse(article_url)
    m = ARTICLE_PATH_RE.match(p.path)
    if not m:
        return "news"
    return m.group(1)  # category segment


# ---------- Styling ----------
def tag_to_icon(tag: str) -> str:
    # –ù–µ–±–æ–ª—å—à–∞—è ‚Äú—Å–µ–º–∞–Ω—Ç–∏–∫–∞‚Äù, –æ—Å—Ç–∞–ª—å–Ω–æ–µ –±—É–¥–µ—Ç üì∞
    mapping = {
        "updates": "üõ†Ô∏è",
        "specials": "üéÅ",
        "general-news": "üì¢",
        "merchandise": "üõçÔ∏è",
        "clan": "üõ°Ô∏è",
        "tournaments": "üèÜ",
        "competitive-gaming": "üèÜ",
        "community": "üë•",
        "live-streams": "üì∫",
        "guides": "üìò",
        "ranked": "üéñÔ∏è",
        "frontline": "üöö",
        "battle-pass": "üéüÔ∏è",
        "common-test": "üß™",
        "test": "üß™",
    }
    return mapping.get(tag, "üì∞")


def tag_to_label(tag: str) -> str:
    # –ü–æ–∫–∞–∂–µ–º ‚Äú–∫–∞–∫ –µ—Å—Ç—å‚Äù, –Ω–æ —á—É—Ç—å –ø—Ä–∏—É–∫—Ä–∞—Å–∏–º –æ—Å–Ω–æ–≤–Ω—ã–µ
    mapping = {
        "updates": "–û–±–Ω–æ–≤–ª–µ–Ω–∏—è",
        "specials": "–ê–∫—Ü–∏–∏",
        "general-news": "–ù–æ–≤–æ—Å—Ç–∏",
        "merchandise": "–ú–µ—Ä—á",
        "common-test": "–¢–µ—Å—Ç",
        "test": "–¢–µ—Å—Ç",
    }
    return mapping.get(tag, tag)


def extra_hashtags_by_title(title: str):
    t = (title or "").lower()
    mapping = [
        (["–ø–∞—Ç—á", "–æ–±–Ω–æ–≤–ª–µ–Ω", "–º–∏–∫—Ä–æ–ø–∞—Ç—á", "update"], "#patch"),
        (["–∞–∫—Ü", "—Å–∫–∏–¥", "—Ä–∞—Å–ø—Ä–æ–¥", "sale", "%"], "#sale"),
        (["–∏–≤–µ–Ω—Ç", "—Å–æ–±—ã—Ç", "event", "–º–∏—Å—Å–∏", "–∑–∞–¥–∞—á"], "#event"),
        (["—Ç—É—Ä–Ω–∏—Ä", "tournament"], "#tournament"),
        (["–ø—Ä–µ–º", "premium"], "#premium"),
        (["—Ç–µ—Å—Ç", "common test", "–æ–±—â–µ–º —Ç–µ—Å—Ç"], "#test"),
        (["–∫–∞—Ä—Ç–∞", "map"], "#maps"),
        (["—Ç–∞–Ω–∫", "–≤–µ—Ç–∫", "branch"], "#tanks"),
    ]
    tags = []
    for keys, tag in mapping:
        if any(k in t for k in keys):
            tags.append(tag)
    # unique keep order
    seen = set()
    out = []
    for x in tags:
        if x not in seen:
            seen.add(x)
            out.append(x)
    return out


def format_dt(dt_utc: datetime | None) -> str:
    if not dt_utc:
        return "‚Äî"
    if ZoneInfo:
        try:
            dt_local = dt_utc.astimezone(ZoneInfo("Europe/Berlin"))
        except Exception:
            dt_local = dt_utc
    else:
        dt_local = dt_utc
    return dt_local.strftime("%d.%m.%Y %H:%M")


def format_caption_style2(title: str, tag: str, published_dt_utc: datetime | None):
    safe_title = html_escape(title or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")
    safe_label = html_escape(tag_to_label(tag))
    safe_tag = html_escape(tag)

    icon = tag_to_icon(tag)
    dt_str = html_escape(format_dt(published_dt_utc))

    extra_tags = extra_hashtags_by_title(title or "")
    extra_tags_str = " ".join(extra_tags)

    hashtags_line = f"üè∑Ô∏è #{safe_tag}"
    if extra_tags_str:
        hashtags_line += f"  {html_escape(extra_tags_str)}"

    return (
        f"üì∞ <b>WoT EU ‚Ä¢ –ù–û–í–û–°–¢–ò</b>\n"
        f"{icon} <b>{safe_title}</b>\n"
        f"üìÖ –î–∞—Ç–∞: <b>{dt_str}</b>\n"
        f"üìå –†–∞–∑–¥–µ–ª: <b>{safe_label}</b>\n"
        f"{hashtags_line}"
    )


# ---------- Parsing index (ALL news) ----------
def parse_news_index_page(page_url: str):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç–∞—Ç—å–∏ (url, rough_title).
    –ó–¥–µ—Å—å –º—ã –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫–∏ —Ñ–æ—Ä–º–∞—Ç–∞ /ru/news/<cat>/<slug>/.
    """
    r = requests.get(page_url, headers={"User-Agent": UA}, timeout=HTTP_TIMEOUT)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "html.parser")

    items = []
    for a in soup.find_all("a", href=True):
        href = a["href"].strip()
        m = ARTICLE_PATH_RE.match(href)
        if not m:
            continue

        url = normalize_url(urljoin(page_url, href))
        title = a.get_text(" ", strip=True) or ""
        # title –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –º–æ–∂–µ—Ç –±—ã—Ç—å —à—É–º–Ω—ã–π ‚Äî –Ω–æ—Ä–º–∞–ª—å–Ω–æ, –º—ã —É—Ç–æ—á–Ω–∏–º –ø–æ —Å—Ç–∞—Ç—å–µ
        items.append((url, title))

    # unique keep order
    seen = set()
    uniq = []
    for url, title in items:
        if url in seen:
            continue
        seen.add(url)
        uniq.append((url, title))

    return uniq


def collect_new_links_all():
    """
    –°–æ–±–∏—Ä–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∏–∑ /ru/news/ + pagination.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ (url, title_guess).
    """
    all_items = []
    for page in range(1, PAGES_TO_SCAN + 1):
        page_url = NEWS_INDEX_URL if page == 1 else urljoin(NEWS_INDEX_URL, f"p{page}/")
        items = parse_news_index_page(page_url)
        all_items.extend(items)

        # —Å—Ç—Ä–∞—Ö–æ–≤–∫–∞: –Ω–µ —Å–æ–±–∏—Ä–∞–µ–º –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ
        if len(all_items) >= (ARTICLES_PER_PAGE_HINT * PAGES_TO_SCAN):
            break

    return all_items


# ---------- Main ----------
def run_once():
    if not BOT_TOKEN or not CHANNEL:
        raise SystemExit("–ù—É–∂–Ω–æ –∑–∞–¥–∞—Ç—å TG_BOT_TOKEN –∏ TG_CHANNEL (–≤ run.bat).")

    init_db()

    now_utc = datetime.now(timezone.utc)
    cutoff = now_utc - timedelta(hours=WINDOW_HOURS)

    # –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –∏–∑ –æ–±—â–µ–π –ª–µ–Ω—Ç—ã
    raw_candidates = collect_new_links_all()
    if not raw_candidates:
        return

    # —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ç–µ, –∫–æ—Ç–æ—Ä—ã—Ö –µ—â—ë –Ω–µ—Ç –≤ –±–∞–∑–µ
    candidates = []
    for url, title_guess in raw_candidates:
        if not already_posted(url):
            candidates.append((url, title_guess))

    if not candidates:
        return

    # –ü—É–±–ª–∏–∫—É–µ–º "–æ—Ç —Å—Ç–∞—Ä—ã—Ö –∫ –Ω–æ–≤—ã–º": —Å–Ω–∞—á–∞–ª–∞ –ø–µ—Ä–µ–≤–µ—Ä–Ω—ë–º —Å–ø–∏—Å–æ–∫
    candidates_ordered = list(reversed(candidates))

    posted_count = 0
    for url, title_guess in candidates_ordered:
        if posted_count >= MAX_POSTS_PER_RUN:
            break

        tag = tag_from_article_url(url)

        try:
            pub_dt, img, title_real = fetch_article_meta(url)
        except Exception:
            # –µ—Å–ª–∏ —Å—Ç–∞—Ç—å—è –Ω–µ –æ—Ç–∫—Ä—ã–ª–∞—Å—å ‚Äî –ø–æ–º–µ—Ç–∏–º –∫–∞–∫ seen, —á—Ç–æ–±—ã –Ω–µ –∑–∞—Ü–∏–∫–ª–∏—Ç—å—Å—è
            mark_posted(url, title_guess or url, tag)
            continue

        # –µ—Å–ª–∏ –¥–∞—Ç—ã –Ω–µ—Ç ‚Äî –Ω–µ –ø–æ—Å—Ç–∏–º (–∏–Ω–∞—á–µ —É–ª–µ—Ç–∏—Ç —Å—Ç–∞—Ä—å—ë), –Ω–æ –ø–æ–º–µ—á–∞–µ–º
        if not pub_dt:
            mark_posted(url, title_real or title_guess or url, tag)
            continue

        # –µ—Å–ª–∏ —Å—Ç–∞—Ä–µ–µ 48—á ‚Äî –Ω–µ –ø–æ—Å—Ç–∏–º, –Ω–æ –ø–æ–º–µ—á–∞–µ–º
        if pub_dt < cutoff:
            mark_posted(url, title_real or title_guess or url, tag)
            continue

        title_to_use = title_real or title_guess or "–ù–æ–≤–æ—Å—Ç—å"
        caption = format_caption_style2(title_to_use, tag, pub_dt)

        if img:
            tg_send_photo(img, caption, button_url=url)
        else:
            # fallback: —Å—Å—ã–ª–∫–∞ –≤ —Ç–µ–∫—Å—Ç–µ –¥–ª—è –ø—Ä–µ–≤—å—é
            safe_url = html_escape(url)
            tg_send_message(
                caption + f"\n\n<a href=\"{safe_url}\">–û—Ç–∫—Ä—ã—Ç—å</a>",
                button_url=url,
            )

        mark_posted(url, title_to_use, tag)
        posted_count += 1
        time.sleep(SLEEP_BETWEEN_POSTS_SEC)


if __name__ == "__main__":
    run_once()